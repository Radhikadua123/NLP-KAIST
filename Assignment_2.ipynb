{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Radhika_20204824_KAIST_AI605_Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83c0d5fc019b49299bf824f0943de00e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_67115d8395a54e3da883d23e8bafc40a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ee5e32791572480ca2df7dab0c8c4e8f",
              "IPY_MODEL_bd640d6eb082428e8825af7500a2ece6"
            ]
          }
        },
        "67115d8395a54e3da883d23e8bafc40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee5e32791572480ca2df7dab0c8c4e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1a6559b235e348a9bbe8fe5e6cbe4437",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4677f1f9795149f8907cc204aec05c56"
          }
        },
        "bd640d6eb082428e8825af7500a2ece6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_42cb57e777e243d1b2d3ee30681833e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.03k/? [00:04&lt;00:00, 1.24kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ff029a85eef44b9f9851f09f89b5c6ff"
          }
        },
        "1a6559b235e348a9bbe8fe5e6cbe4437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4677f1f9795149f8907cc204aec05c56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42cb57e777e243d1b2d3ee30681833e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ff029a85eef44b9f9851f09f89b5c6ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90d87b8f91464cb88236fb71c9b324b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c1c1f6f0ed974c69b0dd0f4f46459c95",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_25fffa0abed4408186dfa035e3ce46c6",
              "IPY_MODEL_4ef9ea163d194529aa71ed4e27a68fd9"
            ]
          }
        },
        "c1c1f6f0ed974c69b0dd0f4f46459c95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25fffa0abed4408186dfa035e3ce46c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b288b8a4bc6646d695248d4c7635d563",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 955,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 955,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b6c1f8bce344bfab33a8ced0c122414"
          }
        },
        "4ef9ea163d194529aa71ed4e27a68fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1fea4d44a4194c30824880d951d5b20b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.19k/? [00:00&lt;00:00, 3.99kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc84f8a29fa94b9db751080a1a12805f"
          }
        },
        "b288b8a4bc6646d695248d4c7635d563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b6c1f8bce344bfab33a8ced0c122414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1fea4d44a4194c30824880d951d5b20b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc84f8a29fa94b9db751080a1a12805f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e549e9179cf44e5954d99ee92ad72e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9eb16ef44cd244c6a25b7123a3c717c8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e0167fa0acaf4138b93d6451d4d2fe88",
              "IPY_MODEL_b320237a714942eea4847b31e71db21e"
            ]
          }
        },
        "9eb16ef44cd244c6a25b7123a3c717c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0167fa0acaf4138b93d6451d4d2fe88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4708377f4b324de3bb71cadb36673ce9",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 8116577,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8116577,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b28bd3ef736741f39f87e69d530d57e2"
          }
        },
        "b320237a714942eea4847b31e71db21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dd53ae79141f42cfa9625a2d0d2bbe0c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30.3M/? [00:01&lt;00:00, 18.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b818a55df4b4b27a8d13a5933b4f726"
          }
        },
        "4708377f4b324de3bb71cadb36673ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b28bd3ef736741f39f87e69d530d57e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd53ae79141f42cfa9625a2d0d2bbe0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b818a55df4b4b27a8d13a5933b4f726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24d172e4f2834f9d84ec5e78bcead238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6564ad58a48646ebb98404867fb9cfca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_87b25d0561fc4b3f9877f4489c99a0a6",
              "IPY_MODEL_9961c3c933354a9aa54de6afce341795"
            ]
          }
        },
        "6564ad58a48646ebb98404867fb9cfca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87b25d0561fc4b3f9877f4489c99a0a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b77d5dce4687420c9fff18cc99fc5494",
            "_dom_classes": [],
            "description": "Downloading: ",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1054280,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1054280,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_51fbf098dd6b4e6fbea97baa73853f64"
          }
        },
        "9961c3c933354a9aa54de6afce341795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eb9dd3589c5649e38d0c395291ff9c41",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4.85M/? [00:00&lt;00:00, 12.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_429e0c1dc45b41e799d6d62392f92208"
          }
        },
        "b77d5dce4687420c9fff18cc99fc5494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "51fbf098dd6b4e6fbea97baa73853f64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb9dd3589c5649e38d0c395291ff9c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "429e0c1dc45b41e799d6d62392f92208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52b9dc35cd604f8a8d4fa404d062cabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6334b402e35e4c368817ef324536219b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_96e45c6287cd4a3886fd21d0bc93d9b3",
              "IPY_MODEL_f5f39086bbcd4a91b28c17b02e3fe6e1"
            ]
          }
        },
        "6334b402e35e4c368817ef324536219b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96e45c6287cd4a3886fd21d0bc93d9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5b6e7c2e02ae492abd766e6a312d61cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c54145f9e7f74e0b85ab78531eafe6fb"
          }
        },
        "f5f39086bbcd4a91b28c17b02e3fe6e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_96040e3f9fce4c97bb00ba12efdbd277",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 87599/0 [00:08&lt;00:00, 17528.97 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f2dceaeb969438085ddf2ed458011e7"
          }
        },
        "5b6e7c2e02ae492abd766e6a312d61cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c54145f9e7f74e0b85ab78531eafe6fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96040e3f9fce4c97bb00ba12efdbd277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f2dceaeb969438085ddf2ed458011e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "333f07e4bbd74dd7a8a32b389b3ef022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1bdd08028fc140d9814aafd1919e2444",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_16dbbc9cbe254a6fab4eec2515d19f17",
              "IPY_MODEL_88ec5c4bcd2644b3b5b7900e555ff1f1"
            ]
          }
        },
        "1bdd08028fc140d9814aafd1919e2444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16dbbc9cbe254a6fab4eec2515d19f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_31fe3149745d4cbd985547568956a7ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f0b998ea3794d9a8a190864c5896fe8"
          }
        },
        "88ec5c4bcd2644b3b5b7900e555ff1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b60246c261eb48738df21c48210b3734",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10570/0 [00:01&lt;00:00, 138.97 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97d5b2e299a8478b8e64bbf46ba1940e"
          }
        },
        "31fe3149745d4cbd985547568956a7ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f0b998ea3794d9a8a190864c5896fe8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b60246c261eb48738df21c48210b3734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97d5b2e299a8478b8e64bbf46ba1940e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReonT_YasRSx"
      },
      "source": [
        "# KAIST AI605 Assignment 2: Token Classification with RNNs and Attention\n",
        "Author: Minjoon Seo (minjoon@kaist.ac.kr)\n",
        "\n",
        "TA in charge: Taehyung Kwon (taehyung.kwon@kaist.ac.kr)\n",
        "\n",
        "**Due date**:  April 19 (Mon) 11:00pm, 2021  \n",
        "\n",
        "\n",
        "Your name: Radhika Dua\n",
        "\n",
        "Your student ID: 20204824\n",
        "\n",
        "Your collaborators: \n",
        "\n",
        "## Assignment Objectives\n",
        "- Verify theoretically and empirically how Transformer's attention mechanism works for sequence modeling task.\n",
        "- Implement Transformer's encoder attention layer from scratch using PyTorch.\n",
        "- Design an Attention-based token classification model using PyTorch.\n",
        "- Apply the token classification model to a popular machine reading comprehension task, Stanford Question Answering Dataset (SQuAD).\n",
        "- (Bonus) Analyze pros and cons between using RNN + attention versus purely attention.\n",
        "\n",
        "## Your Submission\n",
        "Your submission will be a link to a Colab notebook that has all written answers and is fully executable. You will submit your assignment via KLMS. Use in-line LaTeX (see below) for mathematical expressions. Collaboration among students is allowed but it is not a group assignment so make sure your answer and code are your own. Also make sure to mention your collaborators in your assignment with their names and their student ids.\n",
        "\n",
        "## Grading\n",
        "The entire assignment is out of 100 points. There are two bonus questions with 30 points altogether. Your final score can be higher than 100 points.\n",
        "\n",
        "\n",
        "## Environment\n",
        "You will only use Python 3.7 and PyTorch 1.8, which is already available on Colab:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qwta269rqLQ",
        "outputId": "bd9137cc-dafa-4fd7-c36b-5aedce2cabef"
      },
      "source": [
        "from platform import python_version\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"python\", python_version())\n",
        "print(\"torch\", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python 3.7.10\n",
            "torch 1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTm5eq4NwQZs"
      },
      "source": [
        "## 1. Transformer's Attention Layer\n",
        "\n",
        "We will first start with going over a few concepts that you learned in your high school statistics class. The variance of a random variable $X$, $\\text{Var}(X)$ is defined as $\\text{E}[(X-\\mu)^2]$ where $\\mu$ is the mean of $X$. Furthermore, given two independent random variables $X$ and $Y$ and a constant $a$,\n",
        "$$ \\text{Var}(X+Y) = \\text{Var}(X) + \\text{Var}(Y),$$\n",
        "$$ \\text{Var}(aX) = a^2\\text{Var}(X),$$\n",
        "$$ \\text{Var}(XY) = \\text{E}(X^2)\\text{E}(Y^2) - [\\text{E}(X)]^2[\\text{E}(Y)]^2.$$\n",
        "\n",
        "**Problem 1.1** *(10 points)* Suppose we are given two sets of $n$ random variables, $X_1 \\dots X_n$ and $Y_1 \\dots Y_n$, where all of these $2n$ variables are mutually independent and have a mean of $0$ and a variance of $1$. Prove that\n",
        "$$\\text{Var}\\left(\\sum_i^n X_i Y_i\\right) = n.$$\n",
        "\n",
        "### $\\color{blue}{\\text{Solution 1.1}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljR-WH8hSWKp"
      },
      "source": [
        "<font color='blue'>\n",
        "Given two independent random variables $X$ (mean = $\\mu_x$ and standard_deviation = $\\sigma_x$) and $Y$ (mean = $\\mu_y$ and standard_deviation = $\\sigma_y$),  $\\operatorname{Var}(\\mathrm{XY})$ is given by:\n",
        " $$\\operatorname{Var}(\\mathrm{XY}) =\\mathrm{E}\\left[\\mathrm{X}^{2}\\right] \\mathrm{E}\\left[\\mathrm{Y}^{2}\\right]-(\\mathrm{E}[\\mathrm{X}])^{2}(\\mathrm{E}[\\mathrm{Y}])^{2}$$\n",
        " <font color='blue'>   \n",
        "We have $ E(X)=\\mu_x$ and $E(X-\\mu_x)^{2}=\\sigma^{2}$, we can find $E(X^{2})$ using the equation below:\n",
        "\\begin{aligned}E(X^{2})&=E(X-\\mu+\\mu)^{2} \\\\&=E(X-\\mu)^{2}-2 E[(X-\\mu) \\mu]+E\\left(\\mu^{2}\\right) \\\\&=\\sigma^{2}-2 \\mu E(X-\\mu)+\\mu^{2} \\\\&=\\sigma^{2}+\\mu^{2}\\end{aligned}\n",
        "<font color='blue'>\n",
        "Therefore,\n",
        "\\begin{aligned} \\operatorname{Var}(\\mathrm{XY}) &=\\mathrm{E}\\left[\\mathrm{X}^{2}\\right] \\mathrm{E}\\left[\\mathrm{Y}^{2}\\right]-(\\mathrm{E}[\\mathrm{X}])^{2}(\\mathrm{E}[\\mathrm{Y}])^{2} \\\\ &=\\left(\\sigma_{\\mathrm{x}}^{2}+\\mu_{\\mathrm{x}}^{2}\\right)\\left(\\sigma_{\\mathrm{y}}^{2}+\\mu_{\\mathrm{y}}^{2}\\right)-\\mu_{\\mathrm{x}}^{2} \\mu_{\\mathrm{y}}^{2} \\\\ &=\\sigma_{\\mathrm{x}}^{2} \\sigma_{\\mathrm{y}}^{2}+\\mu_{\\mathrm{x}}^{2} \\mu_{\\mathrm{y}}^{2}+\\sigma_{\\mathrm{x}}^{2} \\mu_{\\mathrm{y}}^{2}+\\sigma_{\\mathrm{y}}^{2} \\mu_{\\mathrm{x}}^{2}-\\mu_{\\mathrm{x}}^{2} \\mu_{\\mathrm{y}}^{2} \\\\ &=\\sigma_{\\mathrm{x}}^{2} \\sigma_{\\mathrm{y}}^{2}+\\sigma_{\\mathrm{x}}^{2} \\mu_{\\mathrm{y}}^{2}+\\sigma_{\\mathrm{y}}^{2} \\mu_{\\mathrm{x}}^{2} \\end{aligned}\n",
        "<font color='blue'>    \n",
        "So, we can rewrite $\\operatorname{Var}\\left(\\sum_{i=1}^{n} x_{i} y_{i}\\right)$ as: \n",
        "\\begin{aligned} \\operatorname{Var}\\left(\\sum_{i=1}^{n} x_{i} y_{i}\\right) &=\\sum_{i=1}^{n} \\operatorname{var}\\left(x_{i} y_{i}\\right) \\\\ &=\\sum_{i=1}^{n}\\left(\\sigma_{x_{i}}^{2} \\sigma_{y_{i}}^{2}+\\sigma_{x_{i}}^{2} \\mu_{y_{i}}^{2}+\\sigma_{i}^{2} u_{k}^{2}\\right) \\\\ &=\\sum_{i=1}^{n}\\left(1\\right) \\\\ &=n \\end{aligned} \n",
        "<font color='blue'>\n",
        "Hence proved that $\\operatorname{Var}\\left(\\sum_{i=1}^{n} x_{i} y_{i}\\right)=n$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KIelEM56jn_"
      },
      "source": [
        "In Lecture 08 and 09, we discussed how the attention is computed in Transformer via the following equation,\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.$$\n",
        "**Problem 1.2** *(10 points)*  Suppose $Q$ and $K$ are matrices of independent variables each of which has a mean of $0$ and a variance of $1$. Using what you learned from Problem 1.1., show that\n",
        "$$\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) = 1.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9AJuxUFSWKq"
      },
      "source": [
        "### $\\color{blue}{\\text{Solution 1.2}}$\n",
        "<font color='blue'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebEghLODSWKr"
      },
      "source": [
        "<font color='blue'> **LHS** <font>\n",
        "    <font color='blue'>\n",
        "\\begin{align}\n",
        "\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) &= \\frac{1}{d_k}\\text{Var}\\left(QK^\\top \\right)\n",
        "\\\\ &= \\frac{1}{d_k} (E[(QK^\\top)^2] - E[QK^\\top]^2)\n",
        "\\\\&= \\frac{1}{d_k} (E\\left[ QK^\\top KQ^\\top \\right] - \\left(E[Q] E[K]^\\top \\right)^2)\n",
        "\\\\&= \\frac{1}{d_k} (E\\left[Tr\\left(Q Q^\\top K K^\\top \\right)\\right] - \\left(\\mu_Q^\\top \\mu_K\\right)^2)\n",
        "\\\\&= \\frac{1}{d_k} (Tr\\left( E[Q Q^\\top] E[K K^\\top] \\right) - \\left(\\mu_Q^\\top \\mu_K\\right)^2)\n",
        "\\\\&= \\frac{1}{d_k}( Tr\\left(\n",
        "        \\left( \\mu_Q \\mu_Q^\\top + \\sigma_K \\right)\n",
        "        \\left( \\mu_K \\mu_K^\\top + \\sigma_Q \\right))\n",
        "     \\right) - \\left(\\mu_Q^\\top \\mu_K\\right)^2)\n",
        "\\\\&= \\frac{1}{d_k} (Tr\\left( \\mu_Q \\mu_Q^\\top \\mu_K \\mu_K^\\top \\right)\n",
        "   + Tr\\left( \\mu_Q \\sigma_K \\mu_Q^\\top \\right)\n",
        "   + Tr\\left( \\mu_K \\sigma_Q \\mu_K^\\top \\right)\n",
        "   + Tr\\left( \\sigma_Q \\sigma_K \\right)\n",
        "   - \\left(\\mu_Q^\\top \\mu_K\\right)^2)\n",
        "\\\\&= \\frac{1}{d_k} (\\left(\\mu_Q^\\top \\mu_K\\right)^2\n",
        "   + \\left( \\mu_K \\sigma_Q \\mu_k^\\top \\right)\n",
        "   + \\left( \\mu_Q \\sigma_K \\mu_Q^\\top \\right)\n",
        "   + Tr\\left( \\sigma_Q \\sigma_K \\right)\n",
        "   - \\left(\\mu_Q^\\top \\mu_K\\right)^2)\n",
        "\\\\&= \\frac{1}{d_k} (\\mu_K \\sigma_Q \\mu_X^\\top + \\mu_Q \\sigma_Q \\mu_K^\\top + Tr(\\sigma_Q \\sigma_K))\n",
        "\\\\&= \\frac{1}{d_k} (d_k)\n",
        "\\\\&= 1\n",
        "\\end{align}\n",
        "<font color='blue'>\n",
        "Hence proved that $\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) = 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU3a3FEu6loq"
      },
      "source": [
        "\n",
        "**Problem 1.3** *(10 points)* What would happen if the assumption that the variance of $Q$ and $K$ is $1$ does not hold? Consider each case of it being higher and lower than $1$ and conjecture what it implies, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxf6r7vySWKs"
      },
      "source": [
        "### $\\color{blue}{\\text{Solution 1.3}}$\n",
        "<font color='blue'><br> **Case 1:** If the variance of  𝑄  and  𝐾 is greater than 1, than $\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)$ will also be greater than 1. In this case, the softmax may saturate at initilization making it dificult to learn.  \n",
        "**Case 2:** If the variance of  𝑄  and  𝐾 is less than 1, than $\\text{Var}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)$ will also be less than 1. In this case, the output may be too flat to optimize effectively. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtGo8MAt7By2"
      },
      "source": [
        "## 2. Preprocessing SQuAD\n",
        "\n",
        "We will use `datasets` package offered by Hugging Face, which allows us to easily download various language datasets, including Stanford Question Answering Dataset (SQuAD).\n",
        "\n",
        "First, install the package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEGhK5tO8DcT",
        "outputId": "acbe27e5-fa6a-42aa-8fa4-1be8648e5815"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: fsspec, huggingface-hub, xxhash, datasets\n",
            "Successfully installed datasets-1.6.2 fsspec-2021.4.0 huggingface-hub-0.0.8 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lrJLeSG8Xkl"
      },
      "source": [
        "Then, download SQuAD and print the first example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282,
          "referenced_widgets": [
            "83c0d5fc019b49299bf824f0943de00e",
            "67115d8395a54e3da883d23e8bafc40a",
            "ee5e32791572480ca2df7dab0c8c4e8f",
            "bd640d6eb082428e8825af7500a2ece6",
            "1a6559b235e348a9bbe8fe5e6cbe4437",
            "4677f1f9795149f8907cc204aec05c56",
            "42cb57e777e243d1b2d3ee30681833e2",
            "ff029a85eef44b9f9851f09f89b5c6ff",
            "90d87b8f91464cb88236fb71c9b324b9",
            "c1c1f6f0ed974c69b0dd0f4f46459c95",
            "25fffa0abed4408186dfa035e3ce46c6",
            "4ef9ea163d194529aa71ed4e27a68fd9",
            "b288b8a4bc6646d695248d4c7635d563",
            "6b6c1f8bce344bfab33a8ced0c122414",
            "1fea4d44a4194c30824880d951d5b20b",
            "bc84f8a29fa94b9db751080a1a12805f",
            "5e549e9179cf44e5954d99ee92ad72e7",
            "9eb16ef44cd244c6a25b7123a3c717c8",
            "e0167fa0acaf4138b93d6451d4d2fe88",
            "b320237a714942eea4847b31e71db21e",
            "4708377f4b324de3bb71cadb36673ce9",
            "b28bd3ef736741f39f87e69d530d57e2",
            "dd53ae79141f42cfa9625a2d0d2bbe0c",
            "1b818a55df4b4b27a8d13a5933b4f726",
            "24d172e4f2834f9d84ec5e78bcead238",
            "6564ad58a48646ebb98404867fb9cfca",
            "87b25d0561fc4b3f9877f4489c99a0a6",
            "9961c3c933354a9aa54de6afce341795",
            "b77d5dce4687420c9fff18cc99fc5494",
            "51fbf098dd6b4e6fbea97baa73853f64",
            "eb9dd3589c5649e38d0c395291ff9c41",
            "429e0c1dc45b41e799d6d62392f92208",
            "52b9dc35cd604f8a8d4fa404d062cabd",
            "6334b402e35e4c368817ef324536219b",
            "96e45c6287cd4a3886fd21d0bc93d9b3",
            "f5f39086bbcd4a91b28c17b02e3fe6e1",
            "5b6e7c2e02ae492abd766e6a312d61cb",
            "c54145f9e7f74e0b85ab78531eafe6fb",
            "96040e3f9fce4c97bb00ba12efdbd277",
            "0f2dceaeb969438085ddf2ed458011e7",
            "333f07e4bbd74dd7a8a32b389b3ef022",
            "1bdd08028fc140d9814aafd1919e2444",
            "16dbbc9cbe254a6fab4eec2515d19f17",
            "88ec5c4bcd2644b3b5b7900e555ff1f1",
            "31fe3149745d4cbd985547568956a7ff",
            "6f0b998ea3794d9a8a190864c5896fe8",
            "b60246c261eb48738df21c48210b3734",
            "97d5b2e299a8478b8e64bbf46ba1940e"
          ]
        },
        "id": "ePc6IF9I8Jg1",
        "outputId": "3d70b100-9780-4542-9b07-187cd434d90a"
      },
      "source": [
        "from datasets import load_dataset\n",
        "squad_dataset = load_dataset('squad')\n",
        "print(squad_dataset['train'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83c0d5fc019b49299bf824f0943de00e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1877.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90d87b8f91464cb88236fb71c9b324b9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=955.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e549e9179cf44e5954d99ee92ad72e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=8116577.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24d172e4f2834f9d84ec5e78bcead238",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1054280.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52b9dc35cd604f8a8d4fa404d062cabd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "333f07e4bbd74dd7a8a32b389b3ef022",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a. Subsequent calls will reuse this data.\n",
            "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'id': '5733be284776f41900661182', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIzhbD6a81Q6"
      },
      "source": [
        "Here, `answer_start` corresponds to the character-level start position of the answer, and `text` is the answer text itself. You will note that `answer_start` and `text` fields are given as lists but they only contain one item each. In fact, you can safely assume that this is the case for the training data. During evaluation, however, you will utilize several possible answers so that your evaluation can be compared against all of them. So your code need to handle multiple-answers case as well.\n",
        "\n",
        "As we discussed in Lecture 05, we want to formulate this task as a token classification problem. That is, we want to find which token of the context corresponds to the start position of the answer, and which corresponds to the end.\n",
        "\n",
        "**Problem 2.1** *(10 points)* Write `preprocess()` function that takes a SQuAD example as the input and outputs space-tokenized context and question, as well as the start and end token position of the answer if it has the answer field. That is, a pseudo code would look like:\n",
        "```python\n",
        "def preprocess(example):\n",
        "  out = {'context': ['each', 'token'], \n",
        "         'question': ['each', 'token']}\n",
        "  if 'answers' not in example:\n",
        "    return out\n",
        "  out['answers'] = [{'start': 3, 'end': 5}]\n",
        "  return out\n",
        "```\n",
        "Verify that this code works by comparing between the original answer text and the concatenation of the answer tokens from start to end in training data. Report the percentage of the questions that have exact match."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7pUJgnfJ7Ju"
      },
      "source": [
        "### $\\color{blue}{\\text{Solution 2.1}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyIrug4a7nU6"
      },
      "source": [
        "########## space tokenization #############\n",
        "def tokenization(text): \n",
        "    temp_tokens = text.split(\" \")\n",
        "    return temp_tokens\n",
        "\n",
        "########## function to find start and end of a sublist in a list #############\n",
        "def find_sub_list(sl,l):\n",
        "    results=[]\n",
        "    sll=len(sl)\n",
        "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
        "        if l[ind:ind+sll]==sl:\n",
        "            results.append(ind)\n",
        "            results.append(ind+sll-1)\n",
        "    return results\n",
        "\n",
        "########### preprocess function #############\n",
        "def preprocess(tokenization, example):\n",
        "  out = {}\n",
        "  context = example['context']\n",
        "  question = example['question']\n",
        "  answer = example['answers']['text'][0]\n",
        "  out['context'] = tokenization(context)\n",
        "  out['question'] = tokenization(question)\n",
        "  if 'answers' not in example:\n",
        "    return out\n",
        "  answer_ids = tokenization(answer)\n",
        "  startend = find_sub_list(answer_ids, out['context']) \n",
        "  if(startend):\n",
        "    out['answers'] = [{'start': startend[0], 'end':  startend[1]}]\n",
        "  else:\n",
        "    out['answers'] = []\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyBGpCQR_8TL",
        "outputId": "d94afb39-c60c-41df-f1ba-15a1607fe977"
      },
      "source": [
        "######### An example to show the original and preprocessed data ############\n",
        "print(\"############## EXAMPLE ###############\")\n",
        "i = 0\n",
        "print(squad_dataset['train'][i])\n",
        "output = preprocess(tokenization, squad_dataset['train'][i])\n",
        "orig = squad_dataset['train'][i]['answers']['text'][0]\n",
        "if (output['answers']):\n",
        "  concat = ' '.join(output['context'][output['answers'][0]['start']:output['answers'][0]['end']+1])\n",
        "else:\n",
        "  concat = ''\n",
        "if(orig == concat): \n",
        "  print(\"EXACT MATCH!!!!!\")\n",
        "print(\"Original answer text:\", orig)\n",
        "print(\"Concatenation of the answer tokens from start to end:\", concat)\n",
        "\n",
        "########## Finding percentage of the questions that have exact match ###########\n",
        "exact = 0\n",
        "total = len(squad_dataset['train'])\n",
        "for i in range(total):\n",
        "  output = preprocess(tokenization, squad_dataset['train'][i])\n",
        "  orig = squad_dataset['train'][i]['answers']['text'][0]\n",
        "  if (output['answers']):\n",
        "    concat = ' '.join(output['context'][output['answers'][0]['start']:output['answers'][0]['end']+1])\n",
        "  else:\n",
        "    concat = ''\n",
        "  if(orig == concat): \n",
        "    exact += 1\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"##### Finding percentage of the questions that have exact match #####\")\n",
        "print(\"No of questions that have exact match:\", exact)\n",
        "print(\"Total no of samples:\", total)\n",
        "print(\"Percentage of the questions that have exact match:\", (exact/total)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############## EXAMPLE ###############\n",
            "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'id': '5733be284776f41900661182', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}\n",
            "EXACT MATCH!!!!!\n",
            "Original answer text: Saint Bernadette Soubirous\n",
            "Concatenation of the answer tokens from start to end: Saint Bernadette Soubirous\n",
            "\n",
            "\n",
            "##### Finding percentage of the questions that have exact match #####\n",
            "No of questions that have exact match: 48274\n",
            "Total no of samples: 87599\n",
            "Percentage of the questions that have exact match: 55.10793502208929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SnjtTvhSWKu"
      },
      "source": [
        "<font color='blue'> In the above cell, I presented and example to  verify that this code works by comparing between the original answer text and the concatenation of the answer tokens from start to end in training data. In the above example, the original answer text and the concatenation of the answer tokens from start to end is exactly same. \n",
        "    <font color='blue'>\n",
        "The percentage of the questions that have exact match is **55.108 %**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wikI9OxVCL0E"
      },
      "source": [
        "We want to maximize the percentage of the exact match. You might see a low percentage however, due to bad tokenization. For instance, such space-based tokenization will fail to separate between \"world\" and \"!\" in \"hello world!\". \n",
        "\n",
        "**Problem 2.2** *(10 points)* Write an advanced tokenization model that always separates non-alphabet characters as independent tokens. For instance, \"hello1 world!!\" will be tokenized into \"hello\", \"1\", \"world\", \"!\", and \"!\". Using this new tokenizer, re-run the `preprocess` function and report the exact match percentage. How does the ratio change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ew4lwfxKwjM"
      },
      "source": [
        "### $\\color{blue}{\\text{Solution 2.2}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLSeJz2JvPZw",
        "outputId": "0334fcc4-b7ae-4d3b-c983-45755b87d399"
      },
      "source": [
        "import nltk\n",
        "import regex as re\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "\n",
        "########## advanced tokenization #############\n",
        "def adv_tokenization(text):\n",
        "    tokens = []\n",
        "    regex = re.compile('([^a-zA-Z])')\n",
        "    for s in text.lower().split():\n",
        "        _splitted = re.split(regex, s)\n",
        "        tokens += _splitted\n",
        "    return tokens\n",
        "\n",
        "########## function to find start and end of a sublist in a list #############\n",
        "def find_sub_list(sl,l):\n",
        "    results=[]\n",
        "    sll=len(sl)\n",
        "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
        "        if l[ind:ind+sll]==sl:\n",
        "            results.append(ind)\n",
        "            results.append(ind+sll-1)\n",
        "    return results\n",
        "\n",
        "########### preprocess function #############\n",
        "def preprocess(tokenization, example):\n",
        "  out = {}\n",
        "  context = example['context']\n",
        "  question = example['question']\n",
        "  answer_list = example['answers']['text']\n",
        "  out['context'] = tokenization(context)\n",
        "  out['question'] = tokenization(question)\n",
        "  out['answers'] = example['answers']\n",
        "  out['answer'] = []\n",
        "  out['id'] = example['id']\n",
        "  if 'answers' not in example:\n",
        "    return out\n",
        "    \n",
        "  for i in range(len(answer_list)):\n",
        "\n",
        "    answer = answer_list[i]\n",
        "    answer_ids = tokenization(answer)\n",
        "    startend = find_sub_list(answer_ids, out['context']) \n",
        "    if(startend):\n",
        "      out['answer'].append([{'start': startend[0], 'end':  startend[1]}])\n",
        "      \n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS8yW40CvSHp",
        "outputId": "95cb8177-8ec6-44ce-d4e4-7a4894bb73a3"
      },
      "source": [
        "######### An example to show the original and preprocessed data ############\n",
        "print(\"############## EXAMPLE ###############\")\n",
        "i = 0\n",
        "print(squad_dataset['train'][i])\n",
        "output = preprocess(adv_tokenization, squad_dataset['train'][i])\n",
        "orig = squad_dataset['train'][i]['answers']['text'][0].lower()\n",
        "if (output['answer']):\n",
        "  concat = ' '.join(output['context'][output['answer'][0][0]['start']:output['answer'][0][0]['end']+1])\n",
        "else:\n",
        "  concat = ''\n",
        "if(orig == concat): \n",
        "  print(\"EXACT MATCH!!!!!\")\n",
        "print(\"Original answer text:\", orig)\n",
        "print(\"Concatenation of the answer tokens from start to end:\", concat)\n",
        "\n",
        "########## Finding percentage of the questions that have exact match ###########\n",
        "exact = 0\n",
        "total = len(squad_dataset['train'])\n",
        "for i in range(total):\n",
        "  output = preprocess(adv_tokenization, squad_dataset['train'][i])\n",
        "  orig = squad_dataset['train'][i]['answers']['text'][0].lower()\n",
        "  if (output['answer']):\n",
        "    concat = ' '.join(output['context'][output['answer'][0][0]['start']:output['answer'][0][0]['end']+1])\n",
        "  else:\n",
        "    concat = ''\n",
        "  if(orig == concat): \n",
        "    exact += 1\n",
        "print(\"\\n\")\n",
        "print(\"##### Finding percentage of the questions that have exact match #####\")\n",
        "print(\"No of questions that have exact match:\", exact)\n",
        "print(\"Total no of samples:\", total)\n",
        "print(\"Percentage of the questions that have exact match:\", (exact/total)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############## EXAMPLE ###############\n",
            "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'id': '5733be284776f41900661182', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}\n",
            "EXACT MATCH!!!!!\n",
            "Original answer text: saint bernadette soubirous\n",
            "Concatenation of the answer tokens from start to end: saint bernadette soubirous\n",
            "\n",
            "\n",
            "##### Finding percentage of the questions that have exact match #####\n",
            "No of questions that have exact match: 56311\n",
            "Total no of samples: 87599\n",
            "Percentage of the questions that have exact match: 64.28269729106498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJr41BPm0HtT"
      },
      "source": [
        "<font color='blue'> In the above cell, I presented an example to  verify that this code works by comparing between the original answer text and the concatenation of the answer tokens from start to end in training data. In the above example, the original answer text and the concatenation of the answer tokens from start to end is exactly same. \n",
        "    <font color='blue'>\n",
        "The percentage of the questions that have exact match is **64.2826 %**.  The exact match percentage on using advanced tokenizer increases by **9.17%**. The exact matching percentage is **1.17** times more on using advanced tokenizer than on using space tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6byRsKfnKzzK",
        "outputId": "766d50a4-f3c5-408b-d76b-444204184173"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "########## advanced tokenization #############\n",
        "def adv_tokenization(text): \n",
        "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in nltk.tokenize.word_tokenize(text)]\n",
        "    return tokens\n",
        "\n",
        "########## function to find start and end of a sublist in a list #############\n",
        "def find_sub_list(sl,l):\n",
        "    results=[]\n",
        "    sll=len(sl)\n",
        "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
        "        if l[ind:ind+sll]==sl:\n",
        "            results.append(ind)\n",
        "            results.append(ind+sll-1)\n",
        "    return results\n",
        "\n",
        "########### preprocess function #############\n",
        "def preprocess(tokenization, example):\n",
        "  out = {}\n",
        "  context = example['context']\n",
        "  question = example['question']\n",
        "  answer_list = example['answers']['text']\n",
        "  out['context'] = tokenization(context)\n",
        "  out['question'] = tokenization(question)\n",
        "  out['answers'] = example['answers']\n",
        "  out['answer'] = []\n",
        "  out['id'] = example['id']\n",
        "  if 'answers' not in example:\n",
        "    return out\n",
        "    \n",
        "  for i in range(len(answer_list)):\n",
        "\n",
        "    answer = answer_list[i]\n",
        "    answer_ids = tokenization(answer)\n",
        "    startend = find_sub_list(answer_ids, out['context']) \n",
        "    if(startend):\n",
        "      out['answer'].append([{'start': startend[0], 'end':  startend[1]}])\n",
        "      \n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/radhika/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrnYo2sGOqpJ",
        "outputId": "9399298b-4e91-4474-a9ba-877849cbe8c7"
      },
      "source": [
        "######### An example to show the original and preprocessed data ############\n",
        "print(\"############## EXAMPLE ###############\")\n",
        "i = 0\n",
        "print(squad_dataset['train'][i])\n",
        "output = preprocess(adv_tokenization, squad_dataset['train'][i])\n",
        "orig = squad_dataset['train'][i]['answers']['text'][0].lower()\n",
        "if (output['answer']):\n",
        "  concat = ' '.join(output['context'][output['answer'][0][0]['start']:output['answer'][0][0]['end']+1])\n",
        "else:\n",
        "  concat = ''\n",
        "if(orig == concat): \n",
        "  print(\"EXACT MATCH!!!!!\")\n",
        "print(\"Original answer text:\", orig)\n",
        "print(\"Concatenation of the answer tokens from start to end:\", concat)\n",
        "\n",
        "########## Finding percentage of the questions that have exact match ###########\n",
        "exact = 0\n",
        "total = len(squad_dataset['train'])\n",
        "for i in range(total):\n",
        "  output = preprocess(adv_tokenization, squad_dataset['train'][i])\n",
        "  orig = squad_dataset['train'][i]['answers']['text'][0].lower()\n",
        "  if (output['answer']):\n",
        "    concat = ' '.join(output['context'][output['answer'][0][0]['start']:output['answer'][0][0]['end']+1])\n",
        "  else:\n",
        "    concat = ''\n",
        "  if(orig == concat): \n",
        "    exact += 1\n",
        "print(\"\\n\")\n",
        "print(\"##### Finding percentage of the questions that have exact match #####\")\n",
        "print(\"No of questions that have exact match:\", exact)\n",
        "print(\"Total no of samples:\", total)\n",
        "print(\"Percentage of the questions that have exact match:\", (exact/total)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############## EXAMPLE ###############\n",
            "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'id': '5733be284776f41900661182', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}\n",
            "EXACT MATCH!!!!!\n",
            "Original answer text: saint bernadette soubirous\n",
            "Concatenation of the answer tokens from start to end: saint bernadette soubirous\n",
            "\n",
            "\n",
            "##### Finding percentage of the questions that have exact match #####\n",
            "No of questions that have exact match: 75878\n",
            "Total no of samples: 87599\n",
            "Percentage of the questions that have exact match: 86.6197102706652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Ec01YkSWKw"
      },
      "source": [
        "<font color='blue'> In the above cell, I used more advanced tokenizer and also presented an example to  verify that this code works by comparing between the original answer text and the concatenation of the answer tokens from start to end in training data. In the above example, the original answer text and the concatenation of the answer tokens from start to end is exactly same. \n",
        "    <font color='blue'>\n",
        "The percentage of the questions that have exact match is **86.6197 %**.  The exact match percentage on using advanced tokenizer increases by **31.51%**. The exact matching percentage is **1.57** times more on using advanced tokenizer than on using space tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpEWZl3XYc5M"
      },
      "source": [
        "###### $\\color{blue}{\\text{Constructing vocabulary from train data using advanced tokenizer and adding 'UNK' token}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONik-ylUWcoG",
        "outputId": "086fa14e-2a4e-4591-ada8-237bd2733e17"
      },
      "source": [
        "####### Generating tokens #######\n",
        "total = len(squad_dataset['train'])\n",
        "tokens_context = []; tokens_question = []; tokens = []\n",
        "\n",
        "for i in range(total):\n",
        "  output = preprocess(adv_tokenization, squad_dataset['train'][i])\n",
        "  tokens_context.extend(output['context'])\n",
        "  tokens_question.extend(output['question'])\n",
        "  tokens.extend(output['context']); tokens.extend(output['question'])\n",
        "\n",
        "######## Finding the frequency of occurence of each token ########\n",
        "token_counts = {}\n",
        "tokens_final = []\n",
        "\n",
        "for token in tokens:\n",
        "    if token in token_counts:\n",
        "        token_counts[token] += 1\n",
        "    else:\n",
        "        token_counts[token] = 1\n",
        "\n",
        "####### Constructing vocabulary by including only those tokens that occured atleast 2 times #########\n",
        "for token in token_counts.keys():\n",
        "    if token_counts[token] >= 2:\n",
        "        tokens_final.append(token)\n",
        "\n",
        "vocab = ['PAD'] + ['UNK'] + ['SEP'] + list(set(tokens_final))\n",
        "word2id = {word: id_ for id_, word in enumerate(vocab)}\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "print(word2id[\"UNK\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 97908\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS0hLnKoSWKx",
        "outputId": "0fccabd1-965d-4ce6-8cca-012aac94b6b1"
      },
      "source": [
        "print(word2id['PAD'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue29QoFHHGBc"
      },
      "source": [
        "###### $\\color{blue}{\\text{Preparing train and test data }}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oOHLdYdCjCZ"
      },
      "source": [
        "def prepare_sequence(seq, word2id):\n",
        "    idxs = [word2id[w] if w in word2id else 1 for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "def prepare_labels(context, token_index):\n",
        "    idxs = [1 if i == token_index else 0 for i in range(len(context))]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ20ot1xBXve",
        "outputId": "6908de55-155e-4d20-c888-a31661ab64f4"
      },
      "source": [
        "########## preparing data for training and testing ###########\n",
        "def prepare_data(data):\n",
        "  total = len(data)\n",
        "  context_ids = []\n",
        "  question_ids = []\n",
        "  ques_cont_ids = []\n",
        "  cont_ques_ids = []\n",
        "  labels_start = []\n",
        "  labels_end = []\n",
        "  references = []\n",
        "  context_tokenized = []\n",
        "  ids = []\n",
        "  for i in range(total):\n",
        "    output = preprocess(adv_tokenization, data[i])\n",
        "    output_context = prepare_sequence(output['context'], word2id)\n",
        "    output_question = prepare_sequence(output['question'], word2id)\n",
        "    ques_cont = prepare_sequence(output['question'] + ['SEP']+ output['context'], word2id)\n",
        "    cont_ques = prepare_sequence(output['context'] + ['SEP']+ output['question'], word2id)\n",
        "    \n",
        "    if (output['answer']):\n",
        "      labels_start_per_sample = output['answer'][0][0]['start']\n",
        "      labels_end_per_sample = output['answer'][0][0]['end']+1\n",
        "      if(labels_start_per_sample>0 and labels_end_per_sample<len(output_context) and labels_start_per_sample<labels_end_per_sample):\n",
        "          context_ids.append(output_context)\n",
        "          question_ids.append(output_question)\n",
        "          ques_cont_ids.append(ques_cont)\n",
        "          cont_ques_ids.append(cont_ques)\n",
        "          labels_start.append(labels_start_per_sample)\n",
        "          labels_end.append(labels_end_per_sample)\n",
        "          references.append({'id': output['id'], 'answers': output['answers']})\n",
        "          ids.append(output['id'])\n",
        "          context_tokenized.append(output['context'])\n",
        "          \n",
        "  print(len(labels_end))\n",
        "  return context_ids, question_ids, ques_cont_ids, cont_ques_ids, labels_start, labels_end, references, context_tokenized, ids\n",
        "\n",
        "train_context_ids, train_question_ids, train_ques_cont_ids, train_cont_ques_ids, train_labels_start, train_labels_end, train_references, train_context_tokenized, train_ids = prepare_data(squad_dataset['train'])\n",
        "test_context_ids, test_question_ids, test_ques_cont_ids, test_cont_ques_ids, test_labels_start, test_labels_end, test_references, test_context_tokenized, test_ids = prepare_data(squad_dataset['validation'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "83570\n",
            "10166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJtJXDSvDpE8"
      },
      "source": [
        "## 3. LSTM Baseline for SQuAD\n",
        "\n",
        "We will bring and reuse our model from Assignment 1. There are two key differences, however. First, we need to classify each token instead of the entire sentence. Second, we have two inputs (context and question) instead of just one. \n",
        "\n",
        "Before resolving these differences, you will need to define your evaluation function to correctly evaluate how well your model is doing. Note that the evaluation was very straightforward in Assignment 1's sentiment classification (it is either positive or negative) while it is a bit complicated in SQuAD. We will use the evaluation function provided by `datasets`. You can access to it via the following code.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLMA7wXjHkxP"
      },
      "source": [
        "from datasets import load_metric\n",
        "squad_metric = load_metric('squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7smOVxqXJxyB"
      },
      "source": [
        "You can also easily learn about how to use the function by simply typing the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr7rl9ihHsdx",
        "outputId": "22017f6b-561a-475e-8e63-624873d43de4"
      },
      "source": [
        "squad_metric"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metric(name: \"squad\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
              "Computes SQuAD scores (F1 and EM).\n",
              "Args:\n",
              "    predictions: List of question-answers dictionaries with the following key-values:\n",
              "        - 'id': id of the question-answer pair as given in the references (see below)\n",
              "        - 'prediction_text': the text of the answer\n",
              "    references: List of question-answers dictionaries with the following key-values:\n",
              "        - 'id': id of the question-answer pair (see above),\n",
              "        - 'answers': a Dict in the SQuAD dataset format\n",
              "            {\n",
              "                'text': list of possible texts for the answer, as a list of strings\n",
              "                'answer_start': list of start positions for the answer, as a list of ints\n",
              "            }\n",
              "            Note that answer_start values are not taken into account to compute the metric.\n",
              "Returns:\n",
              "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
              "    'f1': The F-score of predicted tokens versus the gold answer\n",
              "Examples:\n",
              "\n",
              "    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
              "    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
              "    >>> squad_metric = datasets.load_metric(\"squad\")\n",
              "    >>> results = squad_metric.compute(predictions=predictions, references=references)\n",
              "    >>> print(results)\n",
              "    {'exact_match': 100.0, 'f1': 100.0}\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh72l3MtJW6D"
      },
      "source": [
        "**Problem 3.1** *(10 points)* Let's resolve the first issue here. Hence, for now, assume that your only input is context and you want to obtain the answer without seeing the question. While this may seem to be a non-sense, actually it can be considered as modeling the prior $\\text{Prob}(a|c)$ before observing $q$ (we ultimately want $\\text{Prob}(a|q,c)$). Transform your model into a token classification model by imposing $\\text{softmax}$ over the tokens instead of predefined classes. You will need to do this twice for each of start and end. Report the accuracy (using the metric above) on `squad_dataset['validation']`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWEesfdnRB08"
      },
      "source": [
        "### $\\color{blue}{\\text{Solution 3.1}}$\n",
        "<font color='blue'> We use the context as the input to the LSTM model and reported the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-59kuyg-ltL",
        "outputId": "9f207452-f7f4-4634-e2f9-c808ea4e28f2"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe6c00f37b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P-_iwJA3I0o"
      },
      "source": [
        "##### Dataset class ##########\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class Squad_data(Dataset):\n",
        "\n",
        "    def __init__(self, context_ids = None, question_ids = None, ques_cont_ids = None, cont_ques_ids = None, labels_start=None, labels_end=None):\n",
        "        self.context_ids = context_ids\n",
        "        self.question_ids = question_ids\n",
        "        self.ques_cont_ids = ques_cont_ids\n",
        "        self.cont_ques_ids = cont_ques_ids\n",
        "        self.labels_start = labels_start\n",
        "        self.labels_end = labels_end\n",
        "        self.len = len(labels_end)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.context_ids[index], self.question_ids[index], self.ques_cont_ids[index], self.cont_ques_ids[index], self.labels_start[index], self.labels_end[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "def pad_collate(batch):\n",
        "  (context_ids, question_ids, ques_cont_ids, cont_ques_ids, labels_start, labels_end) = zip(*batch)\n",
        "  context_lens = [len(x) for x in context_ids]\n",
        "  question_lens = [len(y) for y in question_ids]\n",
        "  ques_cont_lens = [len(y) for y in ques_cont_ids]\n",
        "  cont_ques_lens = ques_cont_lens\n",
        "\n",
        "  context_pad = pad_sequence(context_ids, batch_first=True, padding_value=0)\n",
        "  question_pad = pad_sequence(question_ids, batch_first=True, padding_value=0)\n",
        "  ques_cont_pad = pad_sequence(ques_cont_ids, batch_first=True, padding_value=0)\n",
        "  cont_ques_pad = pad_sequence(cont_ques_ids, batch_first=True, padding_value=0)\n",
        "  labels_start = torch.Tensor(labels_start)\n",
        "  labels_end = torch.Tensor(labels_end)\n",
        "\n",
        "  return context_pad, question_pad, ques_cont_pad, cont_ques_pad, labels_start, labels_end, context_lens, question_lens, ques_cont_lens, cont_ques_lens\n",
        "\n",
        "train_data = Squad_data(train_context_ids[:80000], train_question_ids[:80000], train_ques_cont_ids[:80000], train_cont_ques_ids[:80000], train_labels_start[:80000], train_labels_end[:80000])\n",
        "val_data = Squad_data(train_context_ids[80000:], train_question_ids[80000:], train_ques_cont_ids[80000:], train_cont_ques_ids[80000:], train_labels_start[80000:], train_labels_end[80000:])\n",
        "test_data = Squad_data(test_context_ids, test_question_ids, test_ques_cont_ids, test_cont_ques_ids, test_labels_start, test_labels_end)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True, collate_fn=pad_collate)\n",
        "val_loader = DataLoader(dataset=val_data, batch_size=64, shuffle=False, collate_fn=pad_collate)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False, collate_fn=pad_collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zxOGRXIFUdv"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size = len(vocab), embedding_dim = 256, hidden_dim = 256, tagset_size = 2):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tagset_size = tagset_size\n",
        "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True)\n",
        "\n",
        "        self.hidden2start = nn.Linear(self.hidden_dim, 1)\n",
        "        self.hidden2end = nn.Linear(self.hidden_dim, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, sentence, lens, mask):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        embeds_packed = pack_padded_sequence(embeds, lens, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.lstm(embeds_packed)\n",
        "        lstm_out, output_lengths = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        tag_start = self.hidden2start(lstm_out)\n",
        "        tag_start = tag_start.squeeze()\n",
        "        mask = mask.type(torch.float32)\n",
        "        masked_tag_start = mask * tag_start + (1 - mask) * -1e30\n",
        "        tag_scores_start = F.log_softmax(masked_tag_start, dim=1)\n",
        "        tag_end = self.hidden2end(lstm_out)\n",
        "        tag_end = tag_end.squeeze()\n",
        "        masked_tag_end = mask * tag_end + (1 - mask) * -1e30\n",
        "        tag_scores_end = F.log_softmax(masked_tag_end, dim=1)\n",
        "\n",
        "        return tag_scores_start, tag_scores_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdrJTqyDs5sG"
      },
      "source": [
        "def train(model, train_loader, val_loader, num_epochs = 12, device=None): \n",
        "    model.to(device)\n",
        "    criterion = nn.NLLLoss(reduction='sum')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    train_loss=[]\n",
        "    val_loss = []\n",
        "   \n",
        "    for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      epoch_loss = 0\n",
        "      for i, (train_context_ids, train_question_ids, train_ques_cont_ids, train_cont_ques_ids, train_labels_start, train_labels_end, context_lens, question_lens, ques_cont_lens, cont_ques_lens) in enumerate(train_loader):\n",
        "          input_tensor = train_context_ids.to(device)\n",
        "          c_mask = torch.zeros_like(input_tensor) != input_tensor\n",
        "          train_labels_start = torch.tensor(train_labels_start, dtype=torch.long).to(device)\n",
        "          train_labels_end = torch.tensor(train_labels_end, dtype=torch.long).to(device)\n",
        "          optimizer.zero_grad()\n",
        "          tag_scores_start, tag_scores_end = model(input_tensor, context_lens, c_mask)\n",
        "          loss = criterion(tag_scores_start, train_labels_start) + criterion(tag_scores_end, train_labels_end)\n",
        "          \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.item()\n",
        "      train_loss.append(round((epoch_loss/len(train_data)), 2))\n",
        "\n",
        "      with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_epoch_loss = 0\n",
        "            for i, (val_context_ids, val_question_ids, val_ques_cont_ids, val_cont_ques_ids, val_labels_start, val_labels_end, val_context_lens, val_question_lens, val_ques_cont_lens, val_cont_ques_lens) in enumerate(val_loader):\n",
        "                input_tensor = val_context_ids.to(device)\n",
        "                c_mask = torch.zeros_like(input_tensor) != input_tensor\n",
        "                val_labels_start = torch.tensor(val_labels_start, dtype=torch.long).to(device)\n",
        "                val_labels_end = torch.tensor(val_labels_end, dtype=torch.long).to(device)\n",
        "                tag_scores_start, tag_scores_end = model(input_tensor, val_context_lens, c_mask)\n",
        "\n",
        "                loss = criterion(tag_scores_start, val_labels_start) + criterion(tag_scores_end, val_labels_end)\n",
        "                val_epoch_loss += loss.item()\n",
        "\n",
        "            val_loss.append(round((val_epoch_loss/len(val_data)),2))\n",
        "      print(\"epoch {}: Training Loss- {:.2f}  Val loss - {:.2f}\".format(epoch, epoch_loss/len(train_data), val_epoch_loss/len(val_data)))\n",
        "    return model, train_loss, val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beC095VhRze5"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Eb9pP9F1WyO"
      },
      "source": [
        "def test(model, test_loader):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    criterion = nn.NLLLoss(reduction='sum')\n",
        "    start_list = []\n",
        "    end_list = []\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        for i, (test_context_ids, test_question_ids, test_ques_cont_ids, test_cont_ques_ids, test_labels_start, test_labels_end, test_context_lens, test_question_lens, test_ques_cont_lens, test_cont_ques_lens) in enumerate(test_loader):\n",
        "            input_tensor = test_context_ids.to(device)\n",
        "            c_mask = torch.zeros_like(input_tensor) != input_tensor\n",
        "            test_labels_start = torch.tensor(test_labels_start, dtype=torch.long).to(device)\n",
        "            test_labels_end = torch.tensor(test_labels_end, dtype=torch.long).to(device)\n",
        "            tag_scores_start, tag_scores_end = model(input_tensor, test_context_lens, c_mask)\n",
        "            values, start_idx = torch.max(tag_scores_start, dim=-1)\n",
        "            values_end, end_idx = torch.max(tag_scores_end, dim=-1)\n",
        "            start_list.extend(start_idx)\n",
        "            end_list.extend(end_idx)\n",
        "            loss = 0\n",
        "            loss = criterion(tag_scores_start, test_labels_start) + criterion(tag_scores_end, test_labels_end)\n",
        "            test_loss += loss.item()\n",
        "        print(\"Test loss: {}\". format(round((test_loss/len(test_data)),2)))\n",
        "    return start_list, end_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOa-oZTWSWK0"
      },
      "source": [
        "def evaluate(start_list, end_list, context_tokenized, references, ids):\n",
        "    predictions = []\n",
        "    for i in range(len(ids)):\n",
        "        if(start_list[i]<end_list[i]):\n",
        "            pred_text = ' '.join(context_tokenized[i][start_list[i].item():end_list[i].item()+1])\n",
        "        else:\n",
        "            pred_text = ' '.join(context_tokenized[i][start_list[i].item():start_list[i].item()+1])\n",
        "        predictions.append({'id': ids[i], 'prediction_text': pred_text})\n",
        "    results = squad_metric.compute(predictions=predictions, references=references)\n",
        "    print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BIjmhfGSWK0",
        "outputId": "d94132cb-b1bb-47d1-e816-e61d1baf7acf"
      },
      "source": [
        "num_epochs = 10\n",
        "model = LSTM()\n",
        "model, train_loss, val_loss = train(model, train_loader, val_loader, num_epochs, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  from ipykernel import kernelapp as app\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  app.launch_new_instance()\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0: Training Loss- 7.90  Val loss - 7.78\n",
            "epoch 1: Training Loss- 7.18  Val loss - 7.73\n",
            "epoch 2: Training Loss- 6.71  Val loss - 7.89\n",
            "epoch 3: Training Loss- 6.27  Val loss - 8.04\n",
            "epoch 4: Training Loss- 5.86  Val loss - 8.38\n",
            "epoch 5: Training Loss- 5.49  Val loss - 8.73\n",
            "epoch 6: Training Loss- 5.17  Val loss - 9.03\n",
            "epoch 7: Training Loss- 4.92  Val loss - 9.62\n",
            "epoch 8: Training Loss- 4.71  Val loss - 9.87\n",
            "epoch 9: Training Loss- 4.54  Val loss - 10.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cTDaTCpvedL",
        "scrolled": false,
        "outputId": "c76077a1-84e6-442d-9239-ea9b1c409816"
      },
      "source": [
        "start_list, end_list = test(model, test_loader) \n",
        "evaluate(start_list, end_list, test_context_tokenized, test_references, test_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 10.83\n",
            "{'exact_match': 2.63623844186504, 'f1': 8.896667938985173}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv6ue0NySWK1"
      },
      "source": [
        "<font color='blue'> In the above cells, we train **LSTM model** on train set and evaluate it on the test set. In this experiment, **input is the context** and we want to obtain the answer without seeing the question.\n",
        "On evaluating the model on test set, we get the following scores:<br> \n",
        "    **exact_match:** 2.63623844186504<br> \n",
        "    **f1:** 8.896667938985173"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr15BA5QKqTd"
      },
      "source": [
        "**Problem 3.2** *(10 points)*  Now let's resolve the second issue, by simply concatenating the two inputs into one sequence. The simplest way would be to append the the question at the start *OR* the end of the context. If you put it at the start, you will need to shift the start and the end positions of the answer accordingly. If you put it at the end, it will be necesary to use bidirectional LSTM for the context to be aware of what is ahead (though it is recommended to use bidirectional LSTM even if the question is appended at the start). Whichever you choose, carry it out and report the accuracy. How does it differ from 3.1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wN0Co8HSWK1"
      },
      "source": [
        "### $\\color{blue}{\\text{Solution 3.2}}$\n",
        "<font color='blue'> Now we append the question at the start of the context and use it as the input to the model.\n",
        "I trained two models namely LSTM and BiLSTM and reported their performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPongSAfSWK1"
      },
      "source": [
        "class LSTM1(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size = len(vocab), embedding_dim = 256, hidden_dim = 256, tagset_size = 2):\n",
        "        super(LSTM1, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tagset_size = tagset_size\n",
        "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True)\n",
        "\n",
        "        self.hidden2start = nn.Linear(self.hidden_dim, 1)\n",
        "        self.hidden2end = nn.Linear(self.hidden_dim, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, sentence, ques_lens, context_lens, ques_cont_lens, mask):\n",
        "        mask = mask.type(torch.float32)\n",
        "        for i in range(len(ques_lens)):\n",
        "            mask[i][:ques_lens[i]] = 0\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        embeds_packed = pack_padded_sequence(embeds, ques_cont_lens, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.lstm(embeds_packed)\n",
        "        lstm_out, output_lengths = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        tag_start = self.hidden2start(lstm_out)\n",
        "        tag_start = tag_start.squeeze()\n",
        "        masked_tag_start = mask * tag_start + (1 - mask) * -1e30\n",
        "        tag_scores_start = F.log_softmax(masked_tag_start, dim=1)\n",
        "        tag_end = self.hidden2end(lstm_out)\n",
        "        tag_end = tag_end.squeeze()\n",
        "        masked_tag_end = mask * tag_end + (1 - mask) * -1e30\n",
        "        tag_scores_end = F.log_softmax(masked_tag_end, dim=1)\n",
        "\n",
        "        return tag_scores_start, tag_scores_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Yq0y8vSWK1"
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size = len(vocab), embedding_dim = 256, hidden_dim = 256, tagset_size = 2):\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tagset_size = tagset_size\n",
        "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True, bidirectional =True)\n",
        "\n",
        "        self.hidden2start = nn.Linear(self.hidden_dim*2, 1)\n",
        "        self.hidden2end = nn.Linear(self.hidden_dim*2, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, sentence, ques_lens, context_lens, ques_cont_lens, mask):\n",
        "        mask = mask.type(torch.float32)\n",
        "        for i in range(len(ques_lens)):\n",
        "            mask[i][:ques_lens[i]] = 0\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        embeds_packed = pack_padded_sequence(embeds, ques_cont_lens, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.lstm(embeds_packed)\n",
        "        lstm_out, output_lengths = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        tag_start = self.hidden2start(lstm_out)\n",
        "        tag_start = tag_start.squeeze()\n",
        "        masked_tag_start = mask * tag_start + (1 - mask) * -1e30\n",
        "        tag_scores_start = F.log_softmax(masked_tag_start, dim=1)\n",
        "        tag_end = self.hidden2end(lstm_out)\n",
        "        tag_end = tag_end.squeeze()\n",
        "        masked_tag_end = mask * tag_end + (1 - mask) * -1e30\n",
        "        tag_scores_end = F.log_softmax(masked_tag_end, dim=1)\n",
        "\n",
        "        return tag_scores_start, tag_scores_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P01cQOz7SWK1"
      },
      "source": [
        "def train(model, train_loader, val_loader, num_epochs = 12, device=None): \n",
        "    model.to(device)\n",
        "    criterion = nn.NLLLoss(reduction='sum')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    train_loss=[]\n",
        "    val_loss = []\n",
        "   \n",
        "    for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      epoch_loss = 0\n",
        "      for i, (train_context_ids, train_question_ids, train_ques_cont_ids, train_cont_ques_ids, train_labels_start, train_labels_end, context_lens, question_lens, ques_cont_lens, cont_ques_lens) in enumerate(train_loader):\n",
        "          input_tensor = train_ques_cont_ids.to(device)\n",
        "          c_mask = torch.zeros_like(input_tensor) != input_tensor\n",
        "          train_labels_start = torch.tensor(train_labels_start, dtype=torch.long).to(device)\n",
        "          train_labels_end = torch.tensor(train_labels_end, dtype=torch.long).to(device)\n",
        "          optimizer.zero_grad()\n",
        "          tag_scores_start, tag_scores_end = model(input_tensor, question_lens, context_lens, ques_cont_lens, c_mask)\n",
        "          question_lens = torch.LongTensor(question_lens).to(device)\n",
        "          loss = criterion(tag_scores_start, train_labels_start+question_lens) + criterion(tag_scores_end, train_labels_end + question_lens)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.item()\n",
        "      epoch_loss = round(epoch_loss/len(train_data), 2)\n",
        "      train_loss.append(epoch_loss)\n",
        "\n",
        "      with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_epoch_loss = 0\n",
        "            for i, (val_context_ids, val_question_ids, val_ques_cont_ids, val_cont_ques_ids, val_labels_start, val_labels_end, val_context_lens, val_question_lens, val_ques_cont_lens, val_cont_ques_lens) in enumerate(val_loader):\n",
        "                input_tensor = val_ques_cont_ids.to(device)\n",
        "                c_mask = torch.zeros_like(input_tensor) != input_tensor\n",
        "                val_labels_start = torch.tensor(val_labels_start, dtype=torch.long).to(device)\n",
        "                val_labels_end = torch.tensor(val_labels_end, dtype=torch.long).to(device)\n",
        "                tag_scores_start, tag_scores_end = model(input_tensor, val_question_lens, val_context_lens, val_ques_cont_lens, c_mask)\n",
        "                val_question_lens = torch.LongTensor(val_question_lens).to(device)\n",
        "                loss = criterion(tag_scores_start, val_labels_start+val_question_lens) + criterion(tag_scores_end, val_labels_end+val_question_lens)\n",
        "                val_epoch_loss += loss.item()\n",
        "\n",
        "            val_epoch_loss = round(val_epoch_loss/len(val_data),2)\n",
        "            val_loss.append(val_epoch_loss)\n",
        "      \n",
        "      print(\"epoch {}: Training Loss- {:.2f}  Val loss - {:.2f}\".format(epoch, epoch_loss, val_epoch_loss))\n",
        "    return model, train_loss, val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z-te566SWK2"
      },
      "source": [
        "def test(model, test_loader):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    start_list = []\n",
        "    end_list = []\n",
        "    criterion = nn.NLLLoss(reduction='sum')\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        for i, (test_context_ids, test_question_ids, test_ques_cont_ids, test_cont_ques_ids, test_labels_start, test_labels_end, test_context_lens, test_question_lens, test_ques_cont_lens, test_cont_ques_lens) in enumerate(test_loader):\n",
        "            input_tensor = test_ques_cont_ids.to(device)\n",
        "            c_mask = torch.zeros_like(input_tensor) != input_tensor\n",
        "            test_labels_start = torch.tensor(test_labels_start, dtype=torch.long).to(device)\n",
        "            test_labels_end = torch.tensor(test_labels_end, dtype=torch.long).to(device)\n",
        "            tag_scores_start, tag_scores_end = model(input_tensor, test_question_lens, test_context_lens, test_ques_cont_lens, c_mask)\n",
        "            start_idx = torch.argmax(tag_scores_start, dim=1)\n",
        "            end_idx = torch.argmax(tag_scores_end, dim=1)\n",
        "            test_question_lens = torch.LongTensor(test_question_lens).to(device)\n",
        "            loss = criterion(tag_scores_start, test_labels_start+test_question_lens) + criterion(tag_scores_end, test_labels_end+test_question_lens)\n",
        "            test_loss += loss.item()\n",
        "            start_list.extend(start_idx-test_question_lens)\n",
        "            end_list.extend(end_idx-test_question_lens)\n",
        "        print(\"Test loss: {}\". format(round((test_loss/len(test_data)),2)))\n",
        "    return start_list, end_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzGTArhPSWK2"
      },
      "source": [
        "#### $\\color{blue}{\\text{Training and testing LSTM model}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2wBiQT8SWK2",
        "outputId": "5a16443b-867c-4090-8f1f-d488adbee51c"
      },
      "source": [
        "num_epochs = 10\n",
        "model1 = LSTM1()\n",
        "model1, train_loss, val_loss = train(model1, train_loader, val_loader, num_epochs, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0: Training Loss- 8.33  Val loss - 8.13\n",
            "epoch 1: Training Loss- 7.51  Val loss - 7.92\n",
            "epoch 2: Training Loss- 6.91  Val loss - 7.78\n",
            "epoch 3: Training Loss- 6.27  Val loss - 7.86\n",
            "epoch 4: Training Loss- 5.64  Val loss - 8.11\n",
            "epoch 5: Training Loss- 4.99  Val loss - 8.44\n",
            "epoch 6: Training Loss- 4.35  Val loss - 9.04\n",
            "epoch 7: Training Loss- 3.74  Val loss - 9.67\n",
            "epoch 8: Training Loss- 3.19  Val loss - 10.38\n",
            "epoch 9: Training Loss- 2.70  Val loss - 11.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISBATgEHSWK2",
        "outputId": "7091ce0d-30f7-4623-da04-56798f63b404"
      },
      "source": [
        "start_list, end_list = test(model1, test_loader)    \n",
        "evaluate(start_list, end_list, test_context_tokenized, test_references, test_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 11.26\n",
            "{'exact_match': 4.642927405075743, 'f1': 13.600260130770142}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWKokWzRancg"
      },
      "source": [
        "<font color='blue'> In the above cells, we train the **LSTM model** on train set and evaluate it on the test set. In this experiment, **we append the question at the start of the context and use it as the input** to the model. \n",
        "On evaluating the model on test set, we get the following scores:<br> \n",
        "    **exact_match:** 4.642927405075743<br> \n",
        "    **f1:** 13.600260130770142<br>\n",
        "<font color='blue'> \n",
        "The exact_match and f1 score suggest that on appending the question at the start of the context and using it as an input to the LSTM outperforms the model that uses only context as an input. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAraQ17wSWK2"
      },
      "source": [
        "#### $\\color{blue}{\\text{Training and testing BiLSTM model}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3zk5cHUSWK3",
        "outputId": "029813af-afce-4159-f922-2e097ec09e02"
      },
      "source": [
        "num_epochs = 10\n",
        "model1 = BiLSTM()\n",
        "model1, train_loss, val_loss = train(model1, train_loader, val_loader, num_epochs, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0: Training Loss- 7.75  Val loss - 7.70\n",
            "epoch 1: Training Loss- 6.83  Val loss - 7.21\n",
            "epoch 2: Training Loss- 6.02  Val loss - 7.09\n",
            "epoch 3: Training Loss- 5.32  Val loss - 7.15\n",
            "epoch 4: Training Loss- 4.64  Val loss - 7.38\n",
            "epoch 5: Training Loss- 3.98  Val loss - 7.77\n",
            "epoch 6: Training Loss- 3.34  Val loss - 8.47\n",
            "epoch 7: Training Loss- 2.75  Val loss - 9.08\n",
            "epoch 8: Training Loss- 2.22  Val loss - 9.99\n",
            "epoch 9: Training Loss- 1.77  Val loss - 11.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac2unhUXSWK3",
        "outputId": "1fc09a84-d8c3-4fa6-bf99-3f1d01f0e7d8"
      },
      "source": [
        "start_list, end_list = test(model1, test_loader)    \n",
        "evaluate(start_list, end_list, test_context_tokenized, test_references, test_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 11.26\n",
            "{'exact_match': 8.115286248278576, 'f1': 18.636382168003795}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLsqctdhdswi"
      },
      "source": [
        "<font color='blue'> In the above cells, we train the **BiLSTM model** on train set and evaluate it on the test set. In this experiment, **we append the question at the start of the context and use it as the input** to the model. \n",
        "On evaluating the model on test set, we get the following scores:<br> \n",
        "    **exact_match:** 8.115286248278576<br> \n",
        "    **f1:** 18.636382168003795<br>\n",
        "<font color='blue'> \n",
        "**The exact_match and f1 score suggest that on using BiLSTM instead of LSTM further improves the performance of the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76NH5MK7L1OG"
      },
      "source": [
        "## 4. LSTM + Attention for SQuAD\n",
        "\n",
        "**Problem 4.1** *(20 points)* Here, we will be appending an attention layer on top of LSTM outputs. We will use a single-head attention sublayer from Transformer. That is, you will implement \n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V,$$\n",
        "where $Q, K, V$ is obtained by the linear transformation of the hidden states of the LSTM outputs $H$, i.e. $Q = HW^Q, K=HW^K, V=HW^V$ ($W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d}$ are trainable weights). Note that the output of $\\text{Attention}$ layer has the same dimension as $H$, so you can directly append your token classification layer on top of it. Report the accuracy and compare it with 3.2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1m43ShHeVgy"
      },
      "source": [
        "### $\\color{blue}{\\text{Solution 4.1}}$\n",
        "<font color='blue'> Now, we will be appending an attention layer on top of LSTM outputs. We will use a single-head attention sublayer from Transformer.\n",
        "I did two experiments:\n",
        "   \n",
        "1.   <font color='blue'>Added attention on top of LSTM outputs and reported its performance\n",
        "\n",
        "2.   <font color='blue'>Added attention on top of BiLSTM outputs and reported its performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyKfNfXUfFdz"
      },
      "source": [
        "#### $\\color{blue}{\\text{Attention on top of LSTM outputs }}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-7L80CSSWK3"
      },
      "source": [
        "import math\n",
        "class LSTM_attention(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size = len(vocab), embedding_dim = 256, hidden_dim = 256):\n",
        "        super(LSTM_attention, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True)\n",
        "\n",
        "        self.hq = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.hv = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        self.hk = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "        \n",
        "        self.hidden2start = nn.Linear(self.hidden_dim, 1)\n",
        "        self.hidden2end = nn.Linear(self.hidden_dim, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scale = 1. / math.sqrt(hidden_dim)\n",
        "\n",
        "    def forward(self, sentence, ques_lens, context_lens, ques_cont_lens, mask):\n",
        "        mask = mask.type(torch.float32)\n",
        "        batch_size = sentence.shape[0]\n",
        "        for i in range(len(ques_lens)):\n",
        "            mask[i][:ques_lens[i]] = 0\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        embeds_packed = pack_padded_sequence(embeds, ques_cont_lens, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.lstm(embeds_packed)\n",
        "        lstm_out, output_lengths = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        q = self.hq(lstm_out).view(batch_size, -1, 1, self.hidden_dim).transpose(1, 2)\n",
        "        k = self.hk(lstm_out).view(batch_size, -1, 1, self.hidden_dim).transpose(1, 2)\n",
        "        v = self.hv(lstm_out).view(batch_size, -1, 1, self.hidden_dim).transpose(1, 2)\n",
        "        scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.hidden_dim)\n",
        "        attn_mask = mask.unsqueeze(1)\n",
        "        \n",
        "        attn_mask = attn_mask.expand(batch_size, mask.shape[1], mask.shape[1])\n",
        "        attn_mask = torch.as_tensor(attn_mask, dtype=torch.bool)\n",
        "        attn_mask = attn_mask.unsqueeze(1)\n",
        "        attn_mask = attn_mask.to(device)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        attn_lstm_out = torch.matmul(attn, v)\n",
        "        attn_lstm_out = attn_lstm_out.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)\n",
        "    \n",
        "        tag_start = self.hidden2start(attn_lstm_out)\n",
        "        tag_start = tag_start.squeeze()\n",
        "        masked_tag_start = mask * tag_start + (1 - mask) * -1e30\n",
        "        tag_scores_start = F.log_softmax(masked_tag_start, dim=1)\n",
        "        tag_end = self.hidden2end(attn_lstm_out)\n",
        "        tag_end = tag_end.squeeze()\n",
        "        masked_tag_end = mask * tag_end + (1 - mask) * -1e30\n",
        "        tag_scores_end = F.log_softmax(masked_tag_end, dim=1)\n",
        "\n",
        "        return tag_scores_start, tag_scores_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkjvQy3DSWK3"
      },
      "source": [
        "def train(model, train_loader, val_loader, num_epochs = 12, device=None): \n",
        "    model.to(device)\n",
        "    criterion = nn.NLLLoss(reduction='sum')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    \n",
        "    train_loss=[]\n",
        "    val_loss = []\n",
        "   \n",
        "    for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      epoch_loss = 0\n",
        "      for i, (train_context_ids, train_question_ids, train_ques_cont_ids, train_cont_ques_ids, train_labels_start, train_labels_end, context_lens, question_lens, ques_cont_lens, cont_ques_lens) in enumerate(train_loader):\n",
        "          input_tensor = train_ques_cont_ids.to(device)\n",
        "          c_mask = torch.zeros_like(input_tensor) != input_tensor\n",
        "          train_labels_start = torch.tensor(train_labels_start, dtype=torch.long).to(device)\n",
        "          train_labels_end = torch.tensor(train_labels_end, dtype=torch.long).to(device)\n",
        "          optimizer.zero_grad()\n",
        "          tag_scores_start, tag_scores_end = model(input_tensor, question_lens, context_lens, ques_cont_lens, c_mask)\n",
        "          question_lens = torch.LongTensor(question_lens).to(device)\n",
        "          loss = criterion(tag_scores_start, train_labels_start+question_lens) + criterion(tag_scores_end, train_labels_end + question_lens)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.item()\n",
        "      epoch_loss = round(epoch_loss/len(train_data), 2)\n",
        "      train_loss.append(epoch_loss)\n",
        "\n",
        "      with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_epoch_loss = 0\n",
        "            for i, (val_context_ids, val_question_ids, val_ques_cont_ids, val_cont_ques_ids, val_labels_start, val_labels_end, val_context_lens, val_question_lens, val_ques_cont_lens, val_cont_ques_lens) in enumerate(val_loader):\n",
        "                input_tensor = val_ques_cont_ids.to(device)\n",
        "                c_mask = torch.zeros_like(input_tensor) != input_tensor\n",
        "                val_labels_start = torch.tensor(val_labels_start, dtype=torch.long).to(device)\n",
        "                val_labels_end = torch.tensor(val_labels_end, dtype=torch.long).to(device)\n",
        "                tag_scores_start, tag_scores_end = model(input_tensor, val_question_lens, val_context_lens, val_ques_cont_lens, c_mask)\n",
        "                val_question_lens = torch.LongTensor(val_question_lens).to(device)\n",
        "                loss = criterion(tag_scores_start, val_labels_start+val_question_lens) + criterion(tag_scores_end, val_labels_end+val_question_lens)\n",
        "                val_epoch_loss += loss.item()\n",
        "\n",
        "            val_epoch_loss = round(val_epoch_loss/len(val_data),2)\n",
        "            val_loss.append(val_epoch_loss)\n",
        "      \n",
        "      print(\"epoch {}: Training Loss- {:.2f}  Val loss - {:.2f}\".format(epoch, epoch_loss, val_epoch_loss))\n",
        "    return model, train_loss, val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mOrP-M5SWK4",
        "outputId": "22adcf7b-fafd-4f29-bb16-88e1c563afd7"
      },
      "source": [
        "num_epochs = 10\n",
        "model2 = LSTM_attention()\n",
        "model2, train_loss, val_loss = train(model2, train_loader, val_loader, num_epochs, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0: Training Loss- 8.16  Val loss - 7.84\n",
            "epoch 1: Training Loss- 7.08  Val loss - 7.42\n",
            "epoch 2: Training Loss- 6.34  Val loss - 7.46\n",
            "epoch 3: Training Loss- 5.62  Val loss - 7.85\n",
            "epoch 4: Training Loss- 4.86  Val loss - 8.41\n",
            "epoch 5: Training Loss- 4.10  Val loss - 9.46\n",
            "epoch 6: Training Loss- 3.38  Val loss - 10.72\n",
            "epoch 7: Training Loss- 2.72  Val loss - 12.20\n",
            "epoch 8: Training Loss- 2.19  Val loss - 14.45\n",
            "epoch 9: Training Loss- 1.77  Val loss - 15.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tezvuO9ZSWK4",
        "outputId": "f66f8efa-e019-4686-8e40-487f8c24e32f"
      },
      "source": [
        "start_list, end_list = test(model2, test_loader)    \n",
        "evaluate(start_list, end_list, test_context_tokenized, test_references, test_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 16.16\n",
            "{'exact_match': 5.439700963997639, 'f1': 14.797131155046463}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ19wLw2gC-V"
      },
      "source": [
        "<font color='blue'> In the above cells, we train the **LSTM (with attention) model** on train set and evaluate it on the test set. In this experiment, **we added attention on top of LSTM outputs**. \n",
        "On evaluating the model on test set, we get the following scores:<br> \n",
        "    **exact_match:** 5.439700963997639<br> \n",
        "    **f1:** 14.797131155046463<br>\n",
        "<font color='blue'> \n",
        "The exact_match and f1 score suggest that on adding attention on top of LSTM outputs significantly  improves the performance of the model when compared with LSTM(without attention) model (in solution 3.2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnyaQULufPMK"
      },
      "source": [
        "#### $\\color{blue}{\\text{Attention on top of BiLSTM outputs }}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBHGP8EiSWK4"
      },
      "source": [
        "class BiLSTM_attention(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size = len(vocab), embedding_dim = 256, hidden_dim = 256, tagset_size = 2):\n",
        "        super(BiLSTM_attention, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tagset_size = tagset_size\n",
        "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True, bidirectional =True)\n",
        "        \n",
        "        self.hq = nn.Linear(self.hidden_dim*2, self.hidden_dim*2)\n",
        "        self.hv = nn.Linear(self.hidden_dim*2, self.hidden_dim*2)\n",
        "        self.hk = nn.Linear(self.hidden_dim*2, self.hidden_dim*2)\n",
        "        \n",
        "        self.hidden2start = nn.Linear(self.hidden_dim*2, 1)\n",
        "        self.hidden2end = nn.Linear(self.hidden_dim*2, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scale = 1. / math.sqrt(2*hidden_dim)\n",
        "\n",
        "    def forward(self, sentence, ques_lens, context_lens, ques_cont_lens, mask):\n",
        "        mask = mask.type(torch.float32)\n",
        "        batch_size = sentence.shape[0]\n",
        "        for i in range(len(ques_lens)):\n",
        "            mask[i][:ques_lens[i]] = 0\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        embeds_packed = pack_padded_sequence(embeds, ques_cont_lens, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.lstm(embeds_packed)\n",
        "        lstm_out, output_lengths = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        q = self.hq(lstm_out).view(batch_size, -1, 1, self.hidden_dim*2).transpose(1, 2)\n",
        "        k = self.hk(lstm_out).view(batch_size, -1, 1, self.hidden_dim*2).transpose(1, 2)\n",
        "        v = self.hv(lstm_out).view(batch_size, -1, 1, self.hidden_dim*2).transpose(1, 2)\n",
        "        \n",
        "        scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.hidden_dim*2)\n",
        "        attn_mask = mask.unsqueeze(1)\n",
        "        \n",
        "        attn_mask = attn_mask.expand(batch_size, mask.shape[1], mask.shape[1])\n",
        "        attn_mask = torch.as_tensor(attn_mask, dtype=torch.bool)\n",
        "        attn_mask = attn_mask.unsqueeze(1)\n",
        "        attn_mask = attn_mask.to(device)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        attn_lstm_out = torch.matmul(attn, v)\n",
        "        attn_lstm_out = attn_lstm_out.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim*2)\n",
        "    \n",
        "    \n",
        "        tag_start = self.hidden2start(attn_lstm_out)\n",
        "        tag_start = tag_start.squeeze()\n",
        "        masked_tag_start = mask * tag_start + (1 - mask) * -1e30\n",
        "        tag_scores_start = F.log_softmax(masked_tag_start, dim=1)\n",
        "        tag_end = self.hidden2end(attn_lstm_out)\n",
        "        tag_end = tag_end.squeeze()\n",
        "        masked_tag_end = mask * tag_end + (1 - mask) * -1e30\n",
        "        tag_scores_end = F.log_softmax(masked_tag_end, dim=1)\n",
        "\n",
        "        return tag_scores_start, tag_scores_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0PaX1KmSWK4",
        "outputId": "9cb0666a-2870-47a5-821c-df112c0aa431"
      },
      "source": [
        "num_epochs = 10\n",
        "model2 = BiLSTM_attention()\n",
        "model2, train_loss, val_loss = train(model2, train_loader, val_loader, num_epochs, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0: Training Loss- 7.50  Val loss - 7.00\n",
            "epoch 1: Training Loss- 6.00  Val loss - 6.53\n",
            "epoch 2: Training Loss- 4.95  Val loss - 6.56\n",
            "epoch 3: Training Loss- 3.92  Val loss - 7.01\n",
            "epoch 4: Training Loss- 2.94  Val loss - 7.98\n",
            "epoch 5: Training Loss- 2.11  Val loss - 9.48\n",
            "epoch 6: Training Loss- 1.55  Val loss - 10.51\n",
            "epoch 7: Training Loss- 1.18  Val loss - 12.17\n",
            "epoch 8: Training Loss- 0.95  Val loss - 12.97\n",
            "epoch 9: Training Loss- 0.81  Val loss - 13.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_HeYCarSWK4",
        "outputId": "0883381f-e1cd-4bfa-d211-4bdbfd000dc9"
      },
      "source": [
        "start_list, end_list = test(model2, test_loader) \n",
        "evaluate(start_list, end_list, test_context_tokenized, test_references, test_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 13.44\n",
            "{'exact_match': 9.669486523706473, 'f1': 22.286058898354636}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMC3PutfhHUb"
      },
      "source": [
        "<font color='blue'> In the above cells, we train the **BiLSTM (with attention) model** on train set and evaluate it on the test set. In this experiment, **we added attention on top of BiLSTM outputs**. \n",
        "On evaluating the model on test set, we get the following scores:<br> \n",
        "    **exact_match:** 9.669486523706473<br> \n",
        "    **f1:** 22.286058898354636<br>\n",
        "<font color='blue'> \n",
        "The exact_match and f1 score suggest that on adding attention on top of BiLSTM outputs significantly  improves the performance of the model when compared with BiLSTM(without attention) model (in solution 3.2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT_N1lJqQFY-"
      },
      "source": [
        "**Problem 4.2** *(10 points)* On top of the attention layer, let's add another layer of (bi-directional) LSTM. So this will look like a *sandwich* where the LSTM is bread and the attention is ham. How does it affect the accuracy? Explain why do you think this happens. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu4TMN9ffdm0"
      },
      "source": [
        "### $\\color{blue}{\\text{Solution 4.2}}$\n",
        "<font color='blue'> Now, on top of the attention layer, we added another layer of (bi-directional) LSTM.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTfv4WIiSWK5"
      },
      "source": [
        "class BiLSTM_attention_BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size = len(vocab), embedding_dim = 128, hidden_dim = 128, tagset_size = 2):\n",
        "        super(BiLSTM_attention_BiLSTM, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tagset_size = tagset_size\n",
        "        self.word_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, batch_first=True, bidirectional =True)\n",
        "        self.lstm1 = nn.LSTM(self.hidden_dim*2, self.hidden_dim*2, batch_first=True, bidirectional =True)\n",
        "        self.hq = nn.Linear(self.hidden_dim*2, self.hidden_dim*2)\n",
        "        self.hv = nn.Linear(self.hidden_dim*2, self.hidden_dim*2)\n",
        "        self.hk = nn.Linear(self.hidden_dim*2, self.hidden_dim*2)\n",
        "        \n",
        "        self.hidden2start = nn.Linear(self.hidden_dim*4, 1)\n",
        "        self.hidden2end = nn.Linear(self.hidden_dim*4, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scale = 1. / math.sqrt(2*hidden_dim)\n",
        "\n",
        "    def forward(self, sentence, ques_lens, context_lens, ques_cont_lens, mask):\n",
        "        mask = mask.type(torch.float32)\n",
        "        batch_size = sentence.shape[0]\n",
        "        for i in range(len(ques_lens)):\n",
        "            mask[i][:ques_lens[i]] = 0\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        embeds_packed = pack_padded_sequence(embeds, ques_cont_lens, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.lstm(embeds_packed)\n",
        "        lstm_out, output_lengths = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        q = self.hq(lstm_out).view(batch_size, -1, 1, self.hidden_dim*2).transpose(1, 2)\n",
        "        k = self.hk(lstm_out).view(batch_size, -1, 1, self.hidden_dim*2).transpose(1, 2)\n",
        "        v = self.hv(lstm_out).view(batch_size, -1, 1, self.hidden_dim*2).transpose(1, 2)\n",
        "        \n",
        "        scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.hidden_dim*2)\n",
        "        attn_mask = mask.unsqueeze(1)\n",
        "        \n",
        "        attn_mask = attn_mask.expand(batch_size, mask.shape[1], mask.shape[1])\n",
        "        attn_mask = torch.as_tensor(attn_mask, dtype=torch.bool)\n",
        "        attn_mask = attn_mask.unsqueeze(1)\n",
        "        attn_mask = attn_mask.to(device)\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        attn_lstm_out = torch.matmul(attn, v)\n",
        "        attn_lstm_out = attn_lstm_out.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim*2)\n",
        "    \n",
        "        attn_lstm_out_packed = pack_padded_sequence(attn_lstm_out, ques_cont_lens, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out1, _ = self.lstm1(attn_lstm_out_packed)\n",
        "        lstm_out1, output_lengths1 = pad_packed_sequence(lstm_out1, batch_first=True)\n",
        "        tag_start = self.hidden2start(lstm_out1)\n",
        "        tag_start = tag_start.squeeze()\n",
        "        masked_tag_start = mask * tag_start + (1 - mask) * -1e30\n",
        "        tag_scores_start = F.log_softmax(masked_tag_start, dim=1)\n",
        "        tag_end = self.hidden2end(lstm_out1)\n",
        "        tag_end = tag_end.squeeze()\n",
        "        masked_tag_end = mask * tag_end + (1 - mask) * -1e30\n",
        "        tag_scores_end = F.log_softmax(masked_tag_end, dim=1)\n",
        "\n",
        "        return tag_scores_start, tag_scores_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqUB08gNSWK5",
        "outputId": "2c0e8641-57e9-4039-a05d-05e39194bde0"
      },
      "source": [
        "num_epochs = 10\n",
        "model2 = BiLSTM_attention_BiLSTM()\n",
        "model2, train_loss, val_loss = train(model2, train_loader, val_loader, num_epochs, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0: Training Loss- 7.78  Val loss - 7.29\n",
            "epoch 1: Training Loss- 6.44  Val loss - 6.71\n",
            "epoch 2: Training Loss- 5.63  Val loss - 6.54\n",
            "epoch 3: Training Loss- 4.89  Val loss - 6.57\n",
            "epoch 4: Training Loss- 4.16  Val loss - 6.78\n",
            "epoch 5: Training Loss- 3.44  Val loss - 7.34\n",
            "epoch 6: Training Loss- 2.77  Val loss - 8.16\n",
            "epoch 7: Training Loss- 2.21  Val loss - 8.95\n",
            "epoch 8: Training Loss- 1.77  Val loss - 10.23\n",
            "epoch 9: Training Loss- 1.45  Val loss - 11.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZJs05xASWK5",
        "outputId": "6d444093-aed0-4753-b271-60b3f3398fab"
      },
      "source": [
        "start_list, end_list = test(model2, test_loader) \n",
        "evaluate(start_list, end_list, test_context_tokenized, test_references, test_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/home/radhika/anaconda3/envs/conda_env_rd/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test loss: 11.49\n",
            "{'exact_match': 11.863072988392682, 'f1': 26.28462934302962}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0D2tzc5kohX"
      },
      "source": [
        "<font color='blue'> In the above cells, we train the **BiLSTM attention BiLSTM model** on train set and evaluate it on the test set. In this experiment, **we added attention on top of BiLSTM outputs followed by another BiLSTM layer**. \n",
        "On evaluating the model on test set, we get the following scores:<br> \n",
        "    **exact_match:** 11.863072988392682<br> \n",
        "    **f1:** 26.28462934302962<br>\n",
        "<font color='blue'> \n",
        "The exact_match and f1 score suggest that on adding attention on top of BiLSTM outputs followed by another BiLSTM layer significantly improves the performance of the mode. This model outperforms all the models trained so far.\n",
        "On adding attention on top of LSTM, outputs generate attended outputs with more focus on input important to answer the question. On passing the attented input through a bilstm layer generates an attended answer. An attented input helps in generating a more accurate answer. Hence, the performance of the model improves significantly (as compared to 4.1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN6XQLYJRrR6"
      },
      "source": [
        "## 5. Attention is All You Need (bonus)\n",
        "\n",
        "**Problem 5.1 (bonus)** *(20 points)*  Implement full Transformer encoder to entirely replace LSTMs. You are allowed to copy and paste code from [*Annotated Transformer*](https://nlp.seas.harvard.edu/2018/04/03/attention.html) (but nowhere else). Report the accuracy and explain what seems to happening with attetion-only model compared to LSTM+Attention model(s). \n",
        "\n",
        "**Problem 5.2 (bonus)** *(10 points)* Replace Transformer's sinusoidal position encoding with a fixed-length (of 256) position embedding. That is, you will create a 256-by-$d$ trainable parameter matrix for the position encoding that replaces the variable-length sinusoidal encoding. What is the clear disdvantage of this approach? Report the accuracy and compare it with 5.1. Note that this also has a clear advantage, as we will see in our future lecture on Pretrained Language Model, and more specifically, BERT (Devlin et al., 2018)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlSejh4ySWK5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}